# LSTM-AD SageMaker Algorithm
The [Time Series Anomaly Detection (LSTM-FCN) Algorithm from AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-4pbvedtnnlphw) performs time series anomaly detection with the Long Short-Term Memory Network for Anomaly Detection (LSTM-AD).
It implements both training and inference from CSV data and supports both CPU and GPU instances.
The training and inference Docker images were built by extending the PyTorch 2.0 Python 3.10 SageMaker containers.

## Model Description
The LSTM-AD model reconstructs the time series with a multivariate LSTM model.
The parameters of the LSTM model are learned on normal data (i.e. on data without anomalies) by minimizing the Mean Squared Error (MSE) between the actual time series and the reconstructed time series.
A multivariate Normal distribution is fitted to the reconstruction errors by Maximum Likelihood Estimation.
The normality score at a given time step is given by the likelihood of the reconstruction errors at that time step under the fitted multivariate Normal distribution.
The lower the likelihood at a given a time step, the more likely the time step is to be an anomaly.

<img src=https://fg-research-assets.s3.eu-west-1.amazonaws.com/lstm-ad-diagram.png style="width:80%;margin-top:60px;margin-bottom:50px"/>

*LSTM-AD architecture (source: [ISBN 978-287587014-8](https://www.esann.org/sites/default/files/proceedings/legacy/es2015-56.pdf))*

**Notes:**
- The algorithm splits the training data into two independent subsets: one subset is used for training the LSTM model, while the other subset is used for calculating the reconstruction errors to which the parameters of the multivariate Normal distribution are fitted. The (optional) validation data accepted by the algorithm is only used for scoring the model, i.e. for calculating the mean squared error between the actual values of the time series in the validation dataset and their reconstructed values generated by the previously trained LSTM model.


- The LSTM-AD model views the multivariate time series as different measurements on the same system. An anomaly is intended as an abnormal behavior of the entire system, not of a single individual measurement. As a result, the LSTM-AD model outputs only one normality score for each time step, representing the likelihood that the overall system is in a normal state at that time step. The LSTM-AD model can also be applied to a univariate time series (i.e. to a single time series). Consider fitting the LSTM-AD model to each individual time series if the time series are not similar to each other and are not related, or if you need to identify the anomalies separately in each time series.  


- The LSTM-AD model reconstructs the time series sequence by sequence. Each sequence is used as input for reconstructing the next sequence. The algorithm uses non-overlapping sequences, whose length can be defined through the `"sequence-length"` hyperparameter. The sequence length (also referred to as window size or number of time steps) should be chosen such that enough sequences are available for training. For instance, if the length of the time series is 10000, consider choosing a sequence length not greater than 100. The LSTM-AD model does not return any output for the first sequence, as there is no previous sequence to use as input.
### Model Resources
- **Paper:** [Long Short Term Memory Networks for Anomaly Detection in Time Series](https://www.esann.org/sites/default/files/proceedings/legacy/es2015-56.pdf).

## SageMaker Algorithm Description
The algorithm implements the model as described above with no changes. 
However, the algorithm defines the normality scores using the Normal log-likelihood instead of the likelihood.

### Training
The training algorithm has two input data channels: `"training"` and `"validation"`. 
The `"training"` channel is mandatory, while the `"validation"` channel is optional.

The training and validation datasets should be provided as CSV files and should only contain normal data (i.e. without anomalies).
Each column of the CSV file represents a time series, while each row represents a time step.
All the time series should have the same length and should not contain missing values.
The CSV file should not contain any index column or column headers. 

See the sample input files `train.csv` and `valid.csv` in the `data/training/` folder.
See `notebook.ipynb` for an example of how to launch a training job.

#### Distributed Training
The algorithm supports multi-GPU training on a single instance, which is implemented through [torch.nn.DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html).
The algorithm does not support multi-node (or distributed) training across multiple instances. 

#### Hyperparameters
The training algorithm takes as input the following hyperparameters:
- `num-layers`: `int`. The number of LSTM layers (default = 2).
- `sequence-length`: `int`. The number of time steps processed by each LSTM layer (default = 50).
- `hidden-size`: `int`. The number of hidden units of each LSTM layer (default = 128).
- `dropout`: `float`. The dropout rate applied after each LSTM layer (default = 0.1).
- `lr`: `float`. The learning rate used for training (default = 0.001).
- `batch-size`: `int`. The batch size used for training (default = 64).
- `epochs`: `int`. The number of training epochs (default = 100).

#### Metrics
The training algorithm logs the following metrics:
- `train_loss`: `float`. Training loss.

If the `"validation"` channel is provided, the training algorithm also logs the following additional metrics:
- `valid_loss`: `float`. Validation loss.

See `notebook.ipynb` for an example of how to launch a hyperparameter tuning job.

### Inference
The inference algorithm takes as input a CSV file containing the time series.
Each column of the CSV file represents a time series, while each row represents a time step.
The CSV file should not contain any index column or column headers.
All the time series should have the same length and should not contain missing values.
See the sample input file `test_data.csv` in the `data/inference/input` folder.

The inference algorithm outputs the normality scores and the reconstructions. 
The normality scores are included in the first column, while the reconstructions are included in the subsequent columns. 

**Note:** The algorithm does not return any output for the first sequence, as there is no previous sequence to use as input.

See the sample output file `batch_predictions.csv` in the `data/inference/output/batch` folder.
See `notebook.ipynb` for an example of how to launch a batch transform job.

#### Endpoints
The algorithm supports only real-time inference endpoints. The inference image is too large to be uploaded to a serverless inference endpoint.

See `notebook.ipynb` for an example of how to deploy the model to an endpoint, invoke the endpoint and process the response.
See the sample output file `real_time_predictions.csv` in the `data/inference/output/real-time` folder.

## References
- P. Malhotra, L. Vig, G. Shroff and P. Agarwal, "Long Short Term Memory Networks for Anomaly Detection in Time Series," In *Esann*, vol. 2015, p. 89. 2015. [ISBN 978-287587014-8](https://www.esann.org/sites/default/files/proceedings/legacy/es2015-56.pdf)
